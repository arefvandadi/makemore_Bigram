{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1A8-VopqVIe8GRjjWIq3E9ZU4WteLM6ri","timestamp":1709406338923}],"authorship_tag":"ABX9TyOobroeAFIbrTlFx9ixnFvo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ynDsEvonfTd9","executionInfo":{"status":"ok","timestamp":1709407773336,"user_tz":300,"elapsed":12,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"b82b05ea-0730-460d-849e-e45eaa7a6fd5"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-03-02 19:29:32--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘input.txt.1’\n","\n","\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n","\n","2024-03-02 19:29:32 (19.6 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n","\n"]}],"source":["# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n","!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"]},{"cell_type":"code","source":["with open(\"input.txt\", 'r', encoding='utf-8') as F:\n","  book = F.read()"],"metadata":{"id":"yxpXyAwnnoLx","executionInfo":{"status":"ok","timestamp":1709407773336,"user_tz":300,"elapsed":8,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["print(book[:100])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YF6HYc-2ockq","executionInfo":{"status":"ok","timestamp":1709407773336,"user_tz":300,"elapsed":8,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"c879f720-a071-4cfb-86c9-102a1ef99c44"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You\n"]}]},{"cell_type":"code","source":["book[:100]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"NCup9d5Son3y","executionInfo":{"status":"ok","timestamp":1709407773337,"user_tz":300,"elapsed":8,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"be2ec9ea-52fc-4617-f992-2862bf761951"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["len(book)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ladeUYf0pCUh","executionInfo":{"status":"ok","timestamp":1709407773337,"user_tz":300,"elapsed":7,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"4a6d3ac5-3dc6-491c-83fc-6d9c77bd6968"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1115394"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["r = sorted(set(book))\n","chars =''.join(r)\n","print(chars)\n","charsize = len(chars)\n","print(charsize)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wxkEb6pqpkvX","executionInfo":{"status":"ok","timestamp":1709407773337,"user_tz":300,"elapsed":6,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"4d9b0bae-cf9e-487d-8339-f78f51075730"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n","65\n"]}]},{"cell_type":"code","source":["## I used rfind() function to create my encoder and deccoder instead of using dictionary which was used in the Original code\n","encoder = lambda c: [chars.rfind(c[i]) for i in range(len(c))]\n","decoder = lambda c: \"\".join([chars[i] for i in c])"],"metadata":{"id":"Zz_cjpltqLLg","executionInfo":{"status":"ok","timestamp":1709407773337,"user_tz":300,"elapsed":5,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print(encoder('Hello There'))\n","print(decoder(encoder('Hello There')))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m9RwnY_HzS_U","executionInfo":{"status":"ok","timestamp":1709407773337,"user_tz":300,"elapsed":5,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"a33eed11-1ce9-4060-94e6-0728cd098d4a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[20, 43, 50, 50, 53, 1, 32, 46, 43, 56, 43]\n","Hello There\n"]}]},{"cell_type":"code","source":["book_digits_List=encoder(book)"],"metadata":{"id":"T-j2pGHH1Edn","executionInfo":{"status":"ok","timestamp":1709407773587,"user_tz":300,"elapsed":255,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["print('First 15 characters in the book:  ',book[:15])\n","print('First 15 codes in the encoded book:  ',book_digits_List[:15])\n","print('\\nLength of the characters in Book:     ',len(book))\n","print('Length of the codes in Encoded Book:  ',len(book_digits_List))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"68YnU8L23BRI","executionInfo":{"status":"ok","timestamp":1709407773587,"user_tz":300,"elapsed":3,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"1495d5aa-38be-424e-982d-346e514c9f9f"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["First 15 characters in the book:   First Citizen:\n","\n","First 15 codes in the encoded book:   [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0]\n","\n","Length of the characters in Book:      1115394\n","Length of the codes in Encoded Book:   1115394\n"]}]},{"cell_type":"code","source":["# Convert Encoded Book from python List to PyTorch Tensor\n","import torch\n","book_digits = torch.tensor(book_digits_List)\n","book_digits[:15]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DkWt8aS8BhK2","executionInfo":{"status":"ok","timestamp":1709407776093,"user_tz":300,"elapsed":2508,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"b53af157-6c8e-4e64-8086-8fc41afbf0cd"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["n=len(book_digits)*9//10\n","Train_data = book_digits[:n]\n","Val_data = book_digits[n:]"],"metadata":{"id":"QfMGIYwmBkX1","executionInfo":{"status":"ok","timestamp":1709407776093,"user_tz":300,"elapsed":6,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["batch_size = 4\n","context = 3\n","\n","def get_batch(x):\n","  Batch_start = torch.randint(0, len(x)-context,(batch_size,))\n","  xb = torch.stack([x[Batch_start[i]:Batch_start[i]+context] for i in range(batch_size)])\n","  yb = torch.stack([x[Batch_start[i]+1:Batch_start[i]+context+1] for i in range(batch_size)])\n","  return xb, yb\n","\n","xb, yb = get_batch(book_digits)\n","print(xb)\n","print(yb)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-h9ojAwvu5Xo","executionInfo":{"status":"ok","timestamp":1709407776093,"user_tz":300,"elapsed":5,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"15b40377-660e-4e52-afcc-5aa3e64194c5"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[59, 57, 11],\n","        [ 2,  1, 34],\n","        [ 1, 58, 53],\n","        [ 0, 27,  1]])\n","tensor([[57, 11,  1],\n","        [ 1, 34, 39],\n","        [58, 53, 59],\n","        [27,  1, 51]])\n"]}]},{"cell_type":"code","source":["# Let's define a Bigram Language model\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# nn.module inheritance is not added\n","# super().__init() is not included yet\n","\n","class NgramLanguageModel(nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","    self.channels = nn.Embedding(charsize,charsize)\n","    #self.parameters = [self.channels.weight]\n","\n","  def forward(self,random_training_batch):\n","    # looks up in the Embedding table created in the constructor to assign weights to each character coming in\n","    logits = self.channels(random_training_batch) # Batch (B) X context (T) X Embedding/Channels (C)\n","    return logits\n","\n","  def LossFunction(self,logits,random_training_batch_nextChar):\n","    logits = logits.view(-1,charsize) # we are doing this since Pytorch functinal.cross_entropy function needs Channels to be assigned to the second dimension\n","    Target = random_training_batch_nextChar.view(-1)\n","    Loss = F.cross_entropy(logits,Target)\n","    return Loss"],"metadata":{"id":"sBD6DCzE7KFV","executionInfo":{"status":"ok","timestamp":1709407776093,"user_tz":300,"elapsed":4,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# NgramLM = NgramLanguageModel()\n","# print(NgramLM.channels.weight.grad)\n","#iter([NgramLM.channels.weight])"],"metadata":{"id":"Ughr_cndaJVw","executionInfo":{"status":"ok","timestamp":1709407776094,"user_tz":300,"elapsed":4,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Create a Ngram Language Model Object\n","NgramLM = NgramLanguageModel()\n","\n","# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(NgramLM.parameters(), lr=1e-3)"],"metadata":{"id":"uO9BhCgtiUwo","executionInfo":{"status":"ok","timestamp":1709407780414,"user_tz":300,"elapsed":4325,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["### TRAINING LOOP ###\n","\n","# define number of iterations to train the data\n","Iterations = 20000\n","# number of batches for each iteration\n","batch_size = 4\n","# How many characters are used to predict the next? Context = 1 --> Bigram model, Context > 1 --> Ngram model\n","context = 2\n","\n","for i in range(Iterations):\n","\n","  # get a random batch of data\n","  xb, yb = get_batch(Train_data)\n","\n","  # forward pass\n","  logits = NgramLM.forward(xb)\n","\n","  # Calculate loss\n","  Loss = NgramLM.LossFunction(logits,yb)\n","\n","  #print(NgramLM.channels.weight.grad)\n","  #Zero all parameter gradients\n","  optimizer.zero_grad(set_to_none=True)\n","  #\n","  #print(NgramLM.channels.weight.grad)\n","\n","  # Backward Path to Calculate new grads\n","  Loss.backward()\n","\n","  # Update the weights in embedding\n","  optimizer.step()\n","\n","\n","print(Loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4WCE490kzOQg","executionInfo":{"status":"ok","timestamp":1709407800283,"user_tz":300,"elapsed":19878,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"8d45acc0-92b7-453d-8601-2c17abe2fb2f"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["2.254117727279663\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"pTaACQdkf0nq","executionInfo":{"status":"ok","timestamp":1709407800283,"user_tz":300,"elapsed":11,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}}},"execution_count":17,"outputs":[]}]}