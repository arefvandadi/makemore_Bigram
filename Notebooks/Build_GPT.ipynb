{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTbSeRrAcetV35NHyWcnMT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## __Build GPT__\n","##### Main Google Colab Link from Andrej Karpathy: https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=Q3k1Czf7LuA9\n","\n","- ##### Youtube Video for Tutorial: https://youtu.be/kCc8FmEb1nY\n","- ##### Github repo for the Youtube Video for Tutorial: https://github.com/karpathy/ng-video-lecture\n","- ##### Youtube Video of CodeAcademy which seem to be based on above video: https://youtu.be/UU1WVnMk4E8\n","- ##### Youtube Video of Makemore Language Model from Andrej Karpathy which helps understand some aspects of the codes used below: https://youtu.be/PaCmpygFfXo?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ\n","\n","\n","#### __Important Functions Used__\n","\n","- nn.Embedding\n","- torch.nn.LayerNorm"],"metadata":{"id":"tzaPyPezu6fT"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FDl8L-ngy1B8","executionInfo":{"status":"ok","timestamp":1708728852966,"user_tz":300,"elapsed":481,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"095eb9cb-0d11-45e4-e4ad-d07648472524"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-02-23 22:54:12--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘input.txt’\n","\n","input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n","\n","2024-02-23 22:54:12 (22.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n","\n"]}],"source":["# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n","!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"]},{"cell_type":"code","source":["# read in the dataset to inspect it\n","with open('input.txt', 'r', encoding='utf-8') as f:\n","  text = f.read()"],"metadata":{"id":"4VhUNW50zESt","executionInfo":{"status":"ok","timestamp":1708728852966,"user_tz":300,"elapsed":1,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# let's look at the first 1000 characters\n","print(text[:1000])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oXWfe6Vq0nSy","executionInfo":{"status":"ok","timestamp":1708728853184,"user_tz":300,"elapsed":219,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"14d969d2-1d8f-4d05-88c6-3f547abc7f08"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n","All:\n","We know't, we know't.\n","\n","First Citizen:\n","Let us kill him, and we'll have corn at our own price.\n","Is't a verdict?\n","\n","All:\n","No more talking on't; let it be done: away, away!\n","\n","Second Citizen:\n","One word, good citizens.\n","\n","First Citizen:\n","We are accounted poor citizens, the patricians good.\n","What authority surfeits on would relieve us: if they\n","would yield us but the superfluity, while it were\n","wholesome, we might guess they relieved us humanely;\n","but they think we are too dear: the leanness that\n","afflicts us, the object of our misery, is as an\n","inventory to particularise their abundance; our\n","sufferance is a gain to them Let us revenge this with\n","our pikes, ere we become rakes: for the gods know I\n","speak this in hunger for bread, not in thirst for revenge.\n","\n","\n"]}]},{"cell_type":"code","source":["# here are all the unique characters that occur in this text\n","chars = sorted(set(text))\n","vocab_size = len(chars)\n","print(''.join(chars))\n","print(vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MSwGyV2Z2Hf1","executionInfo":{"status":"ok","timestamp":1708728853184,"user_tz":300,"elapsed":4,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"4dd5d021-e27b-4342-e97a-9bf06f3254b6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n","65\n"]}]},{"cell_type":"code","source":["# create a mapping/tokenizer from characters to integers\n","# This is a simple tokenizer we are making here. There are many character-level tokenizers available.\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","print(encode(\"hi there\"))\n","print(decode(encode(\"hi there\")))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f-9FvAbV4Ga8","executionInfo":{"status":"ok","timestamp":1708728853184,"user_tz":300,"elapsed":3,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"d9ecb8bf-4b60-4204-9e4a-e903b78ab904"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[46, 47, 1, 58, 46, 43, 56, 43]\n","hi there\n"]}]},{"cell_type":"code","source":["# let's now encode the entire text dataset and store it into a torch.Tensor\n","import torch # we use PyTorch: https://pytorch.org\n","data = torch.tensor(encode(text), dtype=torch.long)\n","print(data.shape, data.dtype)\n","print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7zuD6RUW40bS","executionInfo":{"status":"ok","timestamp":1708728858252,"user_tz":300,"elapsed":5071,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"3b4f56d5-90f1-439b-f724-ab776f29464e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1115394]) torch.int64\n","tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n","        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n","         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n","        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n","         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n","        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n","         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n","        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n","        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n","         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n","         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n","        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n","        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n","         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n","        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n","        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n","        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n","        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n","        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n","        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n","        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n","         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n","         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n","         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n","        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n","        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n","        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n","        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n","        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n","        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n","        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n","         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n","         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n","        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n","        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n","        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n","         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n","        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n","        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n","         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n","        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n","        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n","        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n","        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n","        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n","        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n","        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n","        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n","        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n","         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n","        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n","        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n","        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n","        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n","        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n","        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"]}]},{"cell_type":"code","source":["# Let's now split up the data into train and validation sets\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]"],"metadata":{"id":"fGJKEo5C-fGU","executionInfo":{"status":"ok","timestamp":1708728858253,"user_tz":300,"elapsed":6,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# we are not going to feed the whole data into the model at once. It is computationaly inefficient.\n","# we will select random chunks of data from the training data and feed it into the model instead.\n","block_size = 8\n","train_data[:block_size+1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-CpMr8yzedAR","executionInfo":{"status":"ok","timestamp":1708728858253,"user_tz":300,"elapsed":6,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"0095be06-9218-42fc-f38f-aeb04257c4d1"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["#####Note:\n","##### Keep in Mind that __Encoder-only Transfomers__ take a series of input and predict the next word\n","##### In the above code: __Block_size + 1__ allows for Block_size (8) predictions in a tensor containing Block_size + 1 elements\n","##### you can see this effect in the bottom cell"],"metadata":{"id":"yanSIv_Ch6bj"}},{"cell_type":"code","source":["x = train_data[:block_size]\n","y = train_data[1:block_size+1]\n","for t in range(block_size):\n","    context = x[:t+1]\n","    target = y[t]\n","    print(f\"when input is {context} the target: {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pfio-46-iE0n","executionInfo":{"status":"ok","timestamp":1708728858253,"user_tz":300,"elapsed":5,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"5f7d0d36-f598-4927-f430-a9a63fe3dfe9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["when input is tensor([18]) the target: 47\n","when input is tensor([18, 47]) the target: 56\n","when input is tensor([18, 47, 56]) the target: 57\n","when input is tensor([18, 47, 56, 57]) the target: 58\n","when input is tensor([18, 47, 56, 57, 58]) the target: 1\n","when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n","when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n","when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"]}]},{"cell_type":"code","source":["torch.manual_seed(1337)\n","batch_size = 4 # how many independent sequences will we process in parallel?\n","block_size = 8 # what is the maximum context length for predictions?\n","\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,)) # creates a tensor of random numbers with batch_size size --> basically generates 4 random numbers here in the form of tensors\n","    x = torch.stack([data[i:i+block_size] for i in ix]) # starts from the 4 random numbers created at the previous line and grabs the characters in the book/data from that integer to 8 characters after generating a 4X8 tensor\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # does the exact same thing as the previous line but moves one position ahead to grab all the target positions that Transformer has to predict.\n","    return x, y\n","\n","xb, yb = get_batch('train')\n","print('inputs:')\n","print(xb.shape)\n","print(xb)\n","print('targets:')\n","print(yb.shape)\n","print(yb)\n","\n","print('----')\n","\n","for b in range(batch_size): # batch dimension\n","    for t in range(block_size): # time dimension\n","        context = xb[b, :t+1]\n","        target = yb[b,t]\n","        print(f\"when input is {context.tolist()} the target: {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FdMhfBdOiwpC","executionInfo":{"status":"ok","timestamp":1708728858476,"user_tz":300,"elapsed":227,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"60a78618-430d-4733-ea8b-6ab20528a5d8"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs:\n","torch.Size([4, 8])\n","tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n","        [44, 53, 56,  1, 58, 46, 39, 58],\n","        [52, 58,  1, 58, 46, 39, 58,  1],\n","        [25, 17, 27, 10,  0, 21,  1, 54]])\n","targets:\n","torch.Size([4, 8])\n","tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n","        [53, 56,  1, 58, 46, 39, 58,  1],\n","        [58,  1, 58, 46, 39, 58,  1, 46],\n","        [17, 27, 10,  0, 21,  1, 54, 39]])\n","----\n","when input is [24] the target: 43\n","when input is [24, 43] the target: 58\n","when input is [24, 43, 58] the target: 5\n","when input is [24, 43, 58, 5] the target: 57\n","when input is [24, 43, 58, 5, 57] the target: 1\n","when input is [24, 43, 58, 5, 57, 1] the target: 46\n","when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n","when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n","when input is [44] the target: 53\n","when input is [44, 53] the target: 56\n","when input is [44, 53, 56] the target: 1\n","when input is [44, 53, 56, 1] the target: 58\n","when input is [44, 53, 56, 1, 58] the target: 46\n","when input is [44, 53, 56, 1, 58, 46] the target: 39\n","when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n","when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n","when input is [52] the target: 58\n","when input is [52, 58] the target: 1\n","when input is [52, 58, 1] the target: 58\n","when input is [52, 58, 1, 58] the target: 46\n","when input is [52, 58, 1, 58, 46] the target: 39\n","when input is [52, 58, 1, 58, 46, 39] the target: 58\n","when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n","when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n","when input is [25] the target: 17\n","when input is [25, 17] the target: 27\n","when input is [25, 17, 27] the target: 10\n","when input is [25, 17, 27, 10] the target: 0\n","when input is [25, 17, 27, 10, 0] the target: 21\n","when input is [25, 17, 27, 10, 0, 21] the target: 1\n","when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n","when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"]}]},{"cell_type":"markdown","source":["### Note\n","##### Notice that the 4 X 8 tensor __xb__ above has 32 examples which are independent of each other as far as the transformer is concerned.\n","\n","##### The Bigram Language Model below uses only one character to predict the next character:\n","- Why did we define a getbatch function above to grab 8 consecutive characters (block) i.e __xb__ and __yb__?\n","  - Becasue we are using Bigram language model as a first try. Later we would like to generalize the model to be able to read longer contexts."],"metadata":{"id":"1KR6sr-j6jgl"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","torch.manual_seed(1337)\n","\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        logits = self.token_embedding_table(idx) # (B,T,C)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C) # the reason we are changing the shape of the logits is because F.cross_entropy needs dimensions to be (B,C,T) and we are (B,T,C) and didn't wwant to change that. Threfore, B and T are merged together to make (B*T,C) tensor\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # get the predictions\n","            logits, loss = self(idx) # self(idx) is actually self.forward(idx) here since forward function has __call__ method which automatically calls it.\n","            # focus only on the last time step\n","            ## Notice for Bigram Model we only need the previous character to predict the next. That is why we are using -1 for the second dimension or time\n","            ## However the model is defined so it takes in all the previous characters (i.e. mre than one) so when we can use the model for non-Bigram models laters\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            # torch.multinomial takes a probbaility distribution and uses the indices of the column (Here C column) and generate samples. Since Each column index in B X C here correspond to a character in vocabulary, we are predicting the most probable character based on the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n","\n","m = BigramLanguageModel(vocab_size)\n","logits, loss = m(xb, yb)\n","print(logits.shape)\n","print(loss)\n","\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fuIPlZJV6h1Z","executionInfo":{"status":"ok","timestamp":1708728858476,"user_tz":300,"elapsed":2,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"9970faaa-855f-4a85-f7bf-abfe1b2b7b66"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 65])\n","tensor(4.8786, grad_fn=<NllLossBackward0>)\n","\n","Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"]}]},{"cell_type":"code","source":["# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"],"metadata":{"id":"E5rZpE0vtv1f","executionInfo":{"status":"ok","timestamp":1708728860666,"user_tz":300,"elapsed":2191,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","for steps in range(1000): # increase number of steps for good results...\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = m(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","print(loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nuE2MoplcuBM","executionInfo":{"status":"ok","timestamp":1708729141015,"user_tz":300,"elapsed":203777,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"0f8fd12a-6027-4130-85a8-b8c7c659471f"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["2.455449342727661\n"]}]},{"cell_type":"code","source":["print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5YrhaO2scwZE","executionInfo":{"status":"ok","timestamp":1708729703495,"user_tz":300,"elapsed":151,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"55bf09d3-1f40-4417-91eb-123bbe80fb69"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ny hyedo t, the irthincly;'To wea as hin the tatl tharde th atheillderinge grun r angouthe ive ha hien t s had'stin es ave nge aloard mame; beriry l y.\n","\n","Irooure tenerol! s;\n","He\n","\n","AMerost s sp, isend.\n","Wanothe, ciaslloups:\n","Tht,\n","Houcr.\n","Gelilouew! al:\n","PULARe hme marbetoks, tspsthe aicre RIV:\n","\n","TMERINurimeprsteave, aws h,\n","BERDWanm adelonth d, tllf am;\n","ONEd:\n","\n","S:\n","Borise!\n","HI ceisthesadimesh adove; po ro s ILoy?\n","But salincumacke.\n","THeet deejee IO:\n","Whinthin sious tholantheleagakngere d futhisus the y, ler'sid\n"]}]},{"cell_type":"markdown","source":["## The mathematical trick in self-attention"],"metadata":{"id":"GTvOv3X8zMB8"}},{"cell_type":"code","source":["import torch\n","t = torch.softmax(torch.tensor([0.01,0.6,-0.2,0.9]),dim=-1)\n","t.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7XLpem5ajBYS","executionInfo":{"status":"ok","timestamp":1708786825748,"user_tz":300,"elapsed":114,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"b3c98b0c-906e-4a30-ed93-1ac59ca4680e"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.0154)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["t = torch.softmax(torch.tensor([0.01,0.6,-0.2,0.9])*100,dim=-1)\n","t.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9TfwZGH3AReR","executionInfo":{"status":"ok","timestamp":1708786836911,"user_tz":300,"elapsed":173,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"e03d13db-7212-49cf-dde1-498c320a901b"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.2500)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["9//4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GTwH_P9oAgD1","executionInfo":{"status":"ok","timestamp":1708790425223,"user_tz":300,"elapsed":126,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"8cc7cfc0-cc5b-4eb7-c460-6b8d15fbaafc"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":[],"metadata":{"id":"a23NgFxbAhBA"},"execution_count":null,"outputs":[]}]}