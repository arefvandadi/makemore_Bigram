{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1dzH6aitrwy-smrfBa36GCYn4EWoeDgnO","timestamp":1709481276060},{"file_id":"1A8-VopqVIe8GRjjWIq3E9ZU4WteLM6ri","timestamp":1709406338923}],"authorship_tag":"ABX9TyNizVbO01oo+sjeQs4cW/E6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ynDsEvonfTd9","executionInfo":{"status":"ok","timestamp":1709670580343,"user_tz":300,"elapsed":305,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"78f45994-2756-49e1-f5d1-364310e1ac5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-03-05 20:29:40--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘input.txt.2’\n","\n","\rinput.txt.2           0%[                    ]       0  --.-KB/s               \rinput.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.07s   \n","\n","2024-03-05 20:29:40 (15.7 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n","\n"]}],"source":["# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n","!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"]},{"cell_type":"code","source":["with open(\"input.txt\", 'r', encoding='utf-8') as F:\n","  book = F.read()"],"metadata":{"id":"yxpXyAwnnoLx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(book[:100])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YF6HYc-2ockq","executionInfo":{"status":"ok","timestamp":1709670581743,"user_tz":300,"elapsed":1,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"5765de84-a63f-43fb-ac1a-04b0c1a808a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You\n"]}]},{"cell_type":"code","source":["book[:100]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"NCup9d5Son3y","executionInfo":{"status":"ok","timestamp":1709670583044,"user_tz":300,"elapsed":301,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"da4d1aae-66e1-436d-a77f-939202f6f2bc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["len(book)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ladeUYf0pCUh","executionInfo":{"status":"ok","timestamp":1709670583489,"user_tz":300,"elapsed":2,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"0d5b0d30-a7c7-4861-9417-486d57491de6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1115394"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["r = sorted(set(book))\n","chars =''.join(r)\n","print(chars)\n","charsize = len(chars)\n","print(charsize)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wxkEb6pqpkvX","executionInfo":{"status":"ok","timestamp":1709670583953,"user_tz":300,"elapsed":2,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"e5734323-2d5b-4877-954d-8938e3df7a2f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n","65\n"]}]},{"cell_type":"code","source":["## I used rfind() function to create my encoder and deccoder instead of using dictionary which was used in the Original code\n","encoder = lambda c: [chars.rfind(c[i]) for i in range(len(c))]\n","decoder = lambda c: \"\".join([chars[i] for i in c])"],"metadata":{"id":"Zz_cjpltqLLg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(encoder('Hello There'))\n","print(decoder(encoder('Hello There')))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m9RwnY_HzS_U","executionInfo":{"status":"ok","timestamp":1709670584485,"user_tz":300,"elapsed":3,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"4e2fdb3f-4750-4227-ed18-00776a6f7fd1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[20, 43, 50, 50, 53, 1, 32, 46, 43, 56, 43]\n","Hello There\n"]}]},{"cell_type":"code","source":["book_digits_List=encoder(book)"],"metadata":{"id":"T-j2pGHH1Edn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('First 15 characters in the book:  ',book[:15])\n","print('First 15 codes in the encoded book:  ',book_digits_List[:15])\n","print('\\nLength of the characters in Book:     ',len(book))\n","print('Length of the codes in Encoded Book:  ',len(book_digits_List))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"68YnU8L23BRI","executionInfo":{"status":"ok","timestamp":1709670585231,"user_tz":300,"elapsed":3,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"aeb227d0-1709-4ff5-bb6e-707a9a1bdfe6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First 15 characters in the book:   First Citizen:\n","\n","First 15 codes in the encoded book:   [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0]\n","\n","Length of the characters in Book:      1115394\n","Length of the codes in Encoded Book:   1115394\n"]}]},{"cell_type":"code","source":["# Convert Encoded Book from python List to PyTorch Tensor\n","import torch\n","book_digits = torch.tensor(book_digits_List)\n","book_digits[:15]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DkWt8aS8BhK2","executionInfo":{"status":"ok","timestamp":1709670590343,"user_tz":300,"elapsed":4965,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"62d923b2-72ef-4540-f51d-f245a339f5bb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["n=len(book_digits)*9//10\n","Train_data = book_digits[:n]\n","Val_data = book_digits[n:]"],"metadata":{"id":"QfMGIYwmBkX1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 4\n","context = 3\n","\n","# def get_batch(x):\n","#   Batch_start = torch.randint(0, len(x)-context,(batch_size,))\n","#   xb = torch.stack([x[Batch_start[i]:Batch_start[i]+context] for i in range(batch_size)])\n","#   yb = torch.stack([x[Batch_start[i]+1:Batch_start[i]+context+1] for i in range(batch_size)])\n","#   return xb, yb\n","\n","def get_batch(x):\n","  data = Train_data if x == 'train' else Val_data\n","  Batch_start = torch.randint(0, len(data)-context,(batch_size,))\n","  xb = torch.stack([data[Batch_start[i]:Batch_start[i]+context] for i in range(batch_size)])\n","  yb = torch.stack([data[Batch_start[i]+1:Batch_start[i]+context+1] for i in range(batch_size)])\n","  return xb, yb\n","\n","# # data loading\n","# def get_batch(split):\n","#     # generate a small batch of data of inputs x and targets y\n","#     data = train_data if split == 'train' else val_data\n","#     ix = torch.randint(len(data) - block_size, (batch_size,))\n","#     x = torch.stack([data[i:i+block_size] for i in ix])\n","#     y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","#     x, y = x.to(device), y.to(device)\n","#     return x, y\n","\n","xb, yb = get_batch(book_digits)\n","print(xb)\n","print(yb)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-h9ojAwvu5Xo","executionInfo":{"status":"ok","timestamp":1709670592237,"user_tz":300,"elapsed":115,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"0a18a07d-0253-462c-d7e7-e7dbadd9ff49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[47, 56,  6],\n","        [41, 43,  6],\n","        [47, 41, 46],\n","        [ 1, 59, 52]])\n","tensor([[56,  6,  0],\n","        [43,  6,  1],\n","        [41, 46,  1],\n","        [59, 52, 42]])\n"]}]},{"cell_type":"code","source":["# Let's define a Bigram Language model\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","torch.manual_seed(1337)\n","\n","################ Initial Hyperparameters ###########\n","## Hyperparameter ##\n","#number of embedding for each character --> i.e. C channels entering Self-Attention Head\n","n_embed = 32\n","# number of batches for each iteration\n","batch_size = 4\n","# How many characters are used to predict the next? Context = 1 --> Bigram model, Context > 1 --> Ngram model\n","context = 8\n","# number of Self-Attention heads\n","num_heads = 4\n","# size of Self-Attention head\n","head_size = n_embed//num_heads\n","# number of Transformer (Multi-head Self-Attention + Feed Forward) layers\n","n_layer = 3\n","# dropout\n","dropout = 0.2\n","# Max Number of Iteration to Train the Model\n","max_iters = 5000\n","# Loss Evaluation Interval\n","eval_interval = 100\n","# Number of batches to Mean over for Loss evaluation at every interval\n","eval_iters = 200\n","################ Initial Hyperparameters ###########\n","\n","################ Initial Hyperparameters Multiplied by 4 ###########\n","# ## Hyperparameter ##\n","# #number of embedding for each character --> i.e. C channels entering Self-Attention Head\n","# n_embed = 128\n","# # number of batches for each iteration\n","# batch_size = 16\n","# # How many characters are used to predict the next? Context = 1 --> Bigram model, Context > 1 --> Ngram model\n","# context = 32\n","# # number of Self-Attention heads\n","# num_heads = 16\n","# # size of Self-Attention head\n","# head_size = n_embed//num_heads\n","# # number of Transformer (Multi-head Self-Attention + Feed Forward) layers\n","# n_layer = 12\n","# # dropout\n","# dropout = 0.2\n","# # Max Number of Iteration to Train the Model\n","# max_iters = 5000\n","# # Loss Evaluation Interval\n","# eval_interval = 100\n","# # Number of batches to Mean over for Loss evaluation at every interval\n","# eval_iters = 200\n","################ Initial Hyperparameters Multiplied by 2 ###########\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    NgramLM.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits = NgramLM.forward(X)\n","            Loss = NgramLM.LossFunction(logits,Y)\n","            losses[k] = Loss.item()\n","        out[split] = losses.mean()\n","    NgramLM.train()\n","    return out\n","\n","class Head(nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","    self.key = nn.Linear(n_embed,head_size, bias=False)\n","    self.query = nn.Linear(n_embed,head_size, bias=False)\n","    self.value = nn.Linear(n_embed,head_size, bias=False)\n","    self.register_buffer('tril', torch.tril(torch.ones(context, context)))\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self,x):\n","    B,T,C = x.shape\n","    k = self.key(x) # Batch (B) X (T)_Entry X head_size (C) --- since This will be applied to the output of token_emb + pos_emb which is Batch (B) X (T)_Entry X n_embed (C)\n","    q = self.query(x) # Batch (B) X (T)_Entry X head_size (C) --- since This will be applied to the output of token_emb + pos_emb which is Batch (B) X (T)_Entry X n_embed (C)\n","\n","    qk = q @ k.transpose(-2,-1) * C**0.5 ## (B ,T ,C) X (B, C, T) = (B, T, T)\n","    wei = torch.ones(T,T)\n","    wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n","    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","    wei = F.softmax(wei, dim=-1) # (B, T, T)\n","    wei = self.dropout(wei) # dropout some of the information from previous nodes (I don't know why!!)\n","    # perform the weighted aggregation of the values\n","    v = self.value(x) # Batch (B) X (T)_Entry X head_size (C) --- since This will be applied to the output of token_emb + pos_emb which is Batch (B) X (T)_Entry X n_embed (C)\n","    out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n","    return out # each head takes n_embed and outputs head_size (C) --> Batch (B) X (T)_Entry X head_size (C)\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head() for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embed,n_embed) ## Projection Layer back to the residual path: To me it only helps to match the dimensions to the residual path if different\n","        self.dropout = nn.Dropout(dropout) # introducing dropout rigt before it is added back to the residual path\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out)) ## Projection Layer back to the residual path: To me it only helps to match the dimensions to the residual path if different\n","        return out # outputs n_embed ---> by concatenating all the head_size self-attention Heads outputs\n","\n","class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embed, 4*n_embed), # In Attention is All You Need paper the hidden layers of the Feed Forward are 4 times the input and output (Page 5 of paper)\n","            nn.ReLU(),\n","            nn.Linear(4*n_embed, n_embed), ## Projection Layer back to the residual path: To me it only helps to match the dimensions to the residual path if different\n","            nn.Dropout(dropout) # introducing dropout rigt before it is added back to the residual path\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block: communication block (Multi-Head Self Attention) followed by computation blcok (FeedForward) \"\"\"\n","\n","    def __init__(self):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        self.sa = MultiHeadAttention()\n","        self.ffwd = FeedFoward()\n","        self.ln1 = nn.LayerNorm(n_embed)\n","        self.ln2 = nn.LayerNorm(n_embed)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x)) ## the addition of x in this argument is Resudial Connection. ## Layer norm is applied before Self Attention\n","        x = x + self.ffwd(self.ln2(x)) ## the addition of x in this argument is Resudial Connection ## Layer norm is applied before Feed Forward\n","        return x\n","\n","\n","class NgramLanguageModel(nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","    self.channels = nn.Embedding(charsize,n_embed)\n","    self.pos_embedding = nn.Embedding(context,n_embed) # positional embedding which will be the same for all entries in the batch i.e. for each context\n","    #self.SelfAttnHead = Head()\n","    self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n","    self.ln_final = nn.LayerNorm(n_embed) # Another Layer norm right before the last nn.Linear layer\n","    #self.SelfAttnHead = MultiHeadAttention() # takes n_embd and outputs n_embd\n","    #self.ffw = FeedFoward() # takes n_embd and outputs n_embd --> Allows the NN to think about the communication between tokens coming from Self-Sttention before generating Logits\n","    self.lm_head = nn.Linear(n_embed,charsize)\n","\n","  def forward(self,random_training_batch):\n","    # random_training_batch argument is generated by getbatch function (what we are calling xb in this code)\n","    B,T = random_training_batch.shape\n","    # looks up in the Embedding table created in the constructor to assign weights to each character coming in\n","    token_emb = self.channels(random_training_batch) # Batch (B) X (T)_Entry X n_embed (C)\n","    pos_emb = self.pos_embedding(torch.arange(T)) # (T)_Entry X n_embed (C)\n","    x = token_emb + pos_emb # pos_emb is getting broadcasted across all Batches or first dimension ---> Batch (B) X (T)_Entry X n_embed (C)\n","    x = self.blocks(x) # applies blocks of Multi-Head self-attention and feedforward\n","    x = self.ln_final(x)\n","    logits = self.lm_head(x) # Batch (B) X (T)_Entry X charsize (C=65)\n","    return logits\n","\n","  def LossFunction(self,logits,random_training_batch_nextChar):\n","    logits = logits.view(-1,charsize) # we are doing this since Pytorch functinal.cross_entropy function needs Channels to be assigned to the second dimension\n","    Target = random_training_batch_nextChar.view(-1)\n","    Loss = F.cross_entropy(logits,Target)\n","    return Loss\n","\n","  def generate(self, initiator_token, max_new_tokens):\n","    # idx is (B, T) array of indices in the current context\n","    for _ in range(max_new_tokens):\n","        #crop the input to the generator so it is not larger than context size since positional embedding defined above only accpepts values up to context (T)\n","        initiator_token_cropped = initiator_token[:,-context:]\n","        # get the predictions\n","        logits = self.forward(initiator_token_cropped)\n","        # focus only on the last time step\n","        logits = logits[:, -1, :] # becomes (B, C)\n","        # apply softmax to get probabilities\n","        probs = F.softmax(logits, dim=-1) # (B, C)\n","        # sample from the distribution\n","        initiator_token_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","        # append sampled index to the running sequence\n","        initiator_token = torch.cat((initiator_token, initiator_token_next), dim=1) # (B, T+1)\n","    return initiator_token"],"metadata":{"id":"sBD6DCzE7KFV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NgramLM = NgramLanguageModel()\n","# print(NgramLM.channels.weight.grad)\n","#iter([NgramLM.channels.weight])"],"metadata":{"id":"Ughr_cndaJVw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(1337)\n","\n","# Create a Ngram Language Model Object\n","NgramLM = NgramLanguageModel()\n","\n","# print the number of parameters in the model\n","print(sum(p.numel() for p in NgramLM.parameters())/1e6, 'M parameters')\n","\n","# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(NgramLM.parameters(), lr=1e-3)"],"metadata":{"id":"uO9BhCgtiUwo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### TRAINING LOOP ###\n","\n","torch.manual_seed(1337)\n","\n","for iter in range(max_iters):\n","\n","  # every once in a while evaluate the loss on train and val sets\n","  if iter % eval_interval == 0 or iter == max_iters - 1:\n","      losses = estimate_loss()\n","      print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","  # get a random batch of data\n","  xb, yb = get_batch('train')\n","\n","  # forward pass\n","  logits = NgramLM.forward(xb)\n","\n","  # Calculate loss\n","  Loss = NgramLM.LossFunction(logits,yb)\n","\n","  #print(NgramLM.channels.weight.grad)\n","  #Zero all parameter gradients\n","  optimizer.zero_grad(set_to_none=True)\n","  #\n","  #print(NgramLM.channels.weight.grad)\n","\n","  # Backward Path to Calculate new grads\n","  Loss.backward()\n","\n","  # Update the weights in embedding\n","  optimizer.step()\n","\n","\n","\n","#print(Loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4WCE490kzOQg","executionInfo":{"status":"ok","timestamp":1709670803127,"user_tz":300,"elapsed":207612,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"4e3742b0-4808-4e8c-b3b0-50e5aed25a13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step 0: train loss 4.3183, val loss 4.3209\n","step 100: train loss 3.2347, val loss 3.3059\n","step 200: train loss 3.0054, val loss 3.0578\n","step 300: train loss 2.8560, val loss 2.8975\n","step 400: train loss 2.7783, val loss 2.7752\n","step 500: train loss 2.7553, val loss 2.6705\n","step 600: train loss 2.6994, val loss 2.6587\n","step 700: train loss 2.6406, val loss 2.6211\n","step 800: train loss 2.6027, val loss 2.5881\n","step 900: train loss 2.5710, val loss 2.5847\n","step 1000: train loss 2.5492, val loss 2.5708\n","step 1100: train loss 2.5377, val loss 2.5360\n","step 1200: train loss 2.5084, val loss 2.5384\n","step 1300: train loss 2.5109, val loss 2.5009\n","step 1400: train loss 2.5114, val loss 2.4652\n","step 1500: train loss 2.4568, val loss 2.5054\n","step 1600: train loss 2.4858, val loss 2.5108\n","step 1700: train loss 2.4382, val loss 2.4711\n","step 1800: train loss 2.4416, val loss 2.4711\n","step 1900: train loss 2.4566, val loss 2.4658\n","step 2000: train loss 2.4530, val loss 2.4360\n","step 2100: train loss 2.4628, val loss 2.3945\n","step 2200: train loss 2.4201, val loss 2.4274\n","step 2300: train loss 2.3947, val loss 2.4612\n","step 2400: train loss 2.4393, val loss 2.4140\n","step 2500: train loss 2.4137, val loss 2.4259\n","step 2600: train loss 2.4291, val loss 2.4108\n","step 2700: train loss 2.3910, val loss 2.4080\n","step 2800: train loss 2.3889, val loss 2.3746\n","step 2900: train loss 2.3538, val loss 2.4126\n","step 3000: train loss 2.4096, val loss 2.3827\n","step 3100: train loss 2.3820, val loss 2.3776\n","step 3200: train loss 2.3620, val loss 2.3788\n","step 3300: train loss 2.3351, val loss 2.3714\n","step 3400: train loss 2.3340, val loss 2.3828\n","step 3500: train loss 2.3394, val loss 2.3900\n","step 3600: train loss 2.3378, val loss 2.3222\n","step 3700: train loss 2.3518, val loss 2.3636\n","step 3800: train loss 2.3413, val loss 2.3401\n","step 3900: train loss 2.3515, val loss 2.3669\n","step 4000: train loss 2.3577, val loss 2.3384\n","step 4100: train loss 2.3165, val loss 2.3232\n","step 4200: train loss 2.3295, val loss 2.3422\n","step 4300: train loss 2.3424, val loss 2.3315\n","step 4400: train loss 2.3074, val loss 2.3283\n","step 4500: train loss 2.3428, val loss 2.3112\n","step 4600: train loss 2.3023, val loss 2.2945\n","step 4700: train loss 2.3236, val loss 2.3407\n","step 4800: train loss 2.3255, val loss 2.3267\n","step 4900: train loss 2.2991, val loss 2.2965\n","step 4999: train loss 2.2769, val loss 2.3092\n"]}]},{"cell_type":"code","source":["# 2.7061634063720703 ---> Original model with nn.module\n","# 2.2859156131744385 ---> added n_embed and one layer of nn.linear --> self.lm_head = nn.Linear(n_embed,charsize)\n","# 2.4289321899414062 ---> positional embedding was added, however, since there is no communication between the characters, it doesn't have much impact now\n","# 2.332624912261963 ---> One self_Attention head Added\n","# 2.254960298538208 ---> Multihead Self-Attention added\n","# 2.2056541442871094 ---> one Multihead Self-Attention and Feedforward block added\n","# 2.469857931137085 ---> Multiple Multihead Self-Attention and Feedforward block added ---> Seem like Model is suffering from deepening of the network.\n","# 2.1680893898010254 ---> Added residual path to both Multihead self-attention and Feed Forward. Also, a projection layer was added to both Multihead and Feed Forward\n","# 2.2137882709503174 ---> Layer Norm is applied to both Multihead self attention and feed forward layers. results are getting worse!!!\n","# 2.240596294403076 ---> Added layer norm before the final nn.linear for logits and made the model even worse!!!\n","# 2.424642324447632 ---> Introduced dropout=0.2 in the Feed Forward, Multihead Self-Attention and in the Bigram Model right before \"wei\" variable\n","# 1.961281657218933 ---> Multiplied all Initial hyperparameters by 2\n","# 1.5806180238723755 ---> Multiplied all Initial hyperparameters by 4\n","### Results of the Last run:\n","# What-make that's Romans alt bloody things,\n","# Unfulck guard's shape of the made, my precess but my bloat now, I wreaching think wears,\n","# And the oth opers abook, them the now I have chank.\n","# The clease they heave blowg against madd me,\n","# I to touch to leam.\n","\n","# ROMEO:\n","# AlTio my deach, what my heart too ward: be wut marricle too?\n","\n","# RICK:\n","# You what we\n","# you fourthd: for you demervine, and to me by thee bed,\n","# Ssweet think, fathes, We boon so, bids out toged up too Eughtly to death ink there with here brunhept, breso"],"metadata":{"id":"kS_ZW_XFG73k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(1337)\n","## Generate some text with trained model\n","print(decoder(NgramLM.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"],"metadata":{"id":"pTaACQdkf0nq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709570872602,"user_tz":300,"elapsed":55094,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"01a3d14e-739c-447d-c386-8b7492ec262c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","What-make that's Romans alt bloody things,\n","Unfulck guard's shape of the made, my precess but my bloat now, I wreaching think wears,\n","And the oth opers abook, them the now I have chank.\n","The clease they heave blowg against madd me,\n","I to touch to leam.\n","\n","ROMEO:\n","AlTio my deach, what my heart too ward: be wut marricle too?\n","\n","RICK:\n","You what we\n","you fourthd: for you demervine, and to me by thee bed,\n","Ssweet think, fathes, We boon so, bids out toged up too Eughtly to death ink there with here brunhept, breso\n"]}]},{"cell_type":"markdown","source":["##__My Fully Finished Code for a Bigram Model with Multihead Self-Attention as seen in *Attention is All You Need* paper__\n","\n","#### Send to Github ####"],"metadata":{"id":"8bMubaZuGtPF"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","################ Initial Hyperparameters ###########\n","## Hyperparameter ##\n","#number of embedding for each character --> i.e. C channels entering Self-Attention Head\n","n_embed = 32\n","# number of batches for each iteration\n","batch_size = 4\n","# How many characters are used to predict the next? Context = 1 --> Bigram model, Context > 1 --> Ngram model\n","context = 8\n","# number of Self-Attention heads\n","num_heads = 4\n","# size of Self-Attention head\n","head_size = n_embed//num_heads\n","# number of Transformer (Multi-head Self-Attention + Feed Forward) layers\n","n_layer = 3\n","# dropout\n","dropout = 0.2\n","# Max Number of Iteration to Train the Model\n","max_iters = 5000\n","# Loss Evaluation Interval\n","eval_interval = 100\n","# Number of batches to Mean over for Loss evaluation at every interval\n","eval_iters = 200\n","################ Initial Hyperparameters ###########\n","\n","################ Initial Hyperparameters Multiplied by 4 ###########\n","# ## Hyperparameter ##\n","# #number of embedding for each character --> i.e. C channels entering Self-Attention Head\n","# n_embed = 128\n","# # number of batches for each iteration\n","# batch_size = 16\n","# # How many characters are used to predict the next? Context = 1 --> Bigram model, Context > 1 --> Ngram model\n","# context = 32\n","# # number of Self-Attention heads\n","# num_heads = 16\n","# # size of Self-Attention head\n","# head_size = n_embed//num_heads\n","# # number of Transformer (Multi-head Self-Attention + Feed Forward) layers\n","# n_layer = 12\n","# # dropout\n","# dropout = 0.2\n","# # Max Number of Iteration to Train the Model\n","# max_iters = 5000\n","# # Loss Evaluation Interval\n","# eval_interval = 100\n","# # Number of batches to Mean over for Loss evaluation at every interval\n","# eval_iters = 200\n","################ Initial Hyperparameters Multiplied by 2 ###########\n","\n","\n","\n","torch.manual_seed(1337)\n","\n","\n","# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n","!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","\n","# Opening the tinyshakspeare Book\n","with open(\"input.txt\", 'r', encoding='utf-8') as tinyshkspr:\n","  book = tinyshkspr.read()\n","\n","r = sorted(set(book))\n","chars =''.join(r)\n","# Number of unique characters in the model\n","charsize = len(chars)\n","\n","## I used rfind() function to create my encoder and deccoder instead of using dictionary which was used in the Original code\n","encoder = lambda c: [chars.rfind(c[i]) for i in range(len(c))]\n","decoder = lambda c: \"\".join([chars[i] for i in c])\n","\n","# Encode the Book (Type = Python List)\n","book_digits_List=encoder(book)\n","# Convert Encoded Book from python List to PyTorch Tensor\n","book_digits = torch.tensor(book_digits_List)\n","\n","# Creating Training and Evaluation Data\n","n=len(book_digits)*9//10\n","Train_data = book_digits[:n]\n","Val_data = book_digits[n:]\n","\n","# Define a function to grab random batches from either Training or Evaluation data\n","def get_batch(x):\n","  data = Train_data if x == 'train' else Val_data\n","  Batch_start = torch.randint(0, len(data)-context,(batch_size,))\n","  xb = torch.stack([data[Batch_start[i]:Batch_start[i]+context] for i in range(batch_size)])\n","  yb = torch.stack([data[Batch_start[i]+1:Batch_start[i]+context+1] for i in range(batch_size)])\n","  return xb, yb\n","\n","############# Creating a Transformer Block as Seen in Attention is All You Need paper for a Bigram model ###############\n","\n","# A Class definition for Each Self-Attetion Block\n","class Head(nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","    self.key = nn.Linear(n_embed,head_size, bias=False)\n","    self.query = nn.Linear(n_embed,head_size, bias=False)\n","    self.value = nn.Linear(n_embed,head_size, bias=False)\n","    self.register_buffer('tril', torch.tril(torch.ones(context, context)))\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self,x):\n","    B,T,C = x.shape\n","    k = self.key(x) # Batch (B) X (T)_Entry X head_size (C) --- since This will be applied to the output of token_emb + pos_emb which is Batch (B) X (T)_Entry X n_embed (C)\n","    q = self.query(x) # Batch (B) X (T)_Entry X head_size (C) --- since This will be applied to the output of token_emb + pos_emb which is Batch (B) X (T)_Entry X n_embed (C)\n","\n","    qk = q @ k.transpose(-2,-1) * C**0.5 ## (B ,T ,C) X (B, C, T) = (B, T, T)\n","    wei = torch.ones(T,T)\n","    wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n","    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","    wei = F.softmax(wei, dim=-1) # (B, T, T)\n","    wei = self.dropout(wei) # dropout some of the information from previous nodes (I don't know why!!)\n","    # perform the weighted aggregation of the values\n","    v = self.value(x) # Batch (B) X (T)_Entry X head_size (C) --- since This will be applied to the output of token_emb + pos_emb which is Batch (B) X (T)_Entry X n_embed (C)\n","    out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n","    return out # each head takes n_embed and outputs head_size (C) --> Batch (B) X (T)_Entry X head_size (C)\n","\n","# A Class definition to Bring Multiple Attention blocks together and create a Multihead Attention Block\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head() for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embed,n_embed) ## Projection Layer back to the residual path: To me it only helps to match the dimensions to the residual path if different\n","        self.dropout = nn.Dropout(dropout) # introducing dropout rigt before it is added back to the residual path\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out)) ## Projection Layer back to the residual path: To me it only helps to match the dimensions to the residual path if different\n","        return out # outputs n_embed ---> by concatenating all the head_size self-attention Heads outputs\n","\n","class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embed, 4*n_embed), # In Attention is All You Need paper the hidden layers of the Feed Forward are 4 times the input and output (Page 5 of paper)\n","            nn.ReLU(),\n","            nn.Linear(4*n_embed, n_embed), ## Projection Layer back to the residual path: To me it only helps to match the dimensions to the residual path if different\n","            nn.Dropout(dropout) # introducing dropout rigt before it is added back to the residual path\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block: communication block (Multi-Head Self Attention) followed by computation blcok (FeedForward) \"\"\"\n","\n","    def __init__(self):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        self.sa = MultiHeadAttention()\n","        self.ffwd = FeedFoward()\n","        self.ln1 = nn.LayerNorm(n_embed)\n","        self.ln2 = nn.LayerNorm(n_embed)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x)) ## the addition of x in this argument is Resudial Connection. ## Layer norm is applied before Self Attention\n","        x = x + self.ffwd(self.ln2(x)) ## the addition of x in this argument is Resudial Connection ## Layer norm is applied before Feed Forward\n","        return x\n","#\n","#\n","#\n","############## Creating a Bigram Model (It is named Ngram here though but it is Bigram !!!!!) ###################\n","\n","class NgramLanguageModel(nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","    self.channels = nn.Embedding(charsize,n_embed)\n","    self.pos_embedding = nn.Embedding(context,n_embed) # positional embedding which will be the same for all entries in the batch i.e. for each context\n","    #self.SelfAttnHead = Head()\n","    self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n","    self.ln_final = nn.LayerNorm(n_embed) # Another Layer norm right before the last nn.Linear layer\n","    #self.SelfAttnHead = MultiHeadAttention() # takes n_embd and outputs n_embd\n","    #self.ffw = FeedFoward() # takes n_embd and outputs n_embd --> Allows the NN to think about the communication between tokens coming from Self-Sttention before generating Logits\n","    self.lm_head = nn.Linear(n_embed,charsize)\n","\n","  def forward(self,random_training_batch):\n","    # random_training_batch argument is generated by getbatch function (what we are calling xb in this code)\n","    B,T = random_training_batch.shape\n","    # looks up in the Embedding table created in the constructor to assign weights to each character coming in\n","    token_emb = self.channels(random_training_batch) # Batch (B) X (T)_Entry X n_embed (C)\n","    pos_emb = self.pos_embedding(torch.arange(T)) # (T)_Entry X n_embed (C)\n","    x = token_emb + pos_emb # pos_emb is getting broadcasted across all Batches or first dimension ---> Batch (B) X (T)_Entry X n_embed (C)\n","    x = self.blocks(x) # applies blocks of Multi-Head self-attention and feedforward\n","    x = self.ln_final(x)\n","    logits = self.lm_head(x) # Batch (B) X (T)_Entry X charsize (C=65)\n","    return logits\n","\n","  def LossFunction(self,logits,random_training_batch_nextChar):\n","    logits = logits.view(-1,charsize) # we are doing this since Pytorch functinal.cross_entropy function needs Channels to be assigned to the second dimension\n","    Target = random_training_batch_nextChar.view(-1)\n","    Loss = F.cross_entropy(logits,Target)\n","    return Loss\n","\n","  def generate(self, initiator_token, max_new_tokens):\n","    # idx is (B, T) array of indices in the current context\n","    for _ in range(max_new_tokens):\n","        #crop the input to the generator so it is not larger than context size since positional embedding defined above only accpepts values up to context (T)\n","        initiator_token_cropped = initiator_token[:,-context:]\n","        # get the predictions\n","        logits = self.forward(initiator_token_cropped)\n","        # focus only on the last time step\n","        logits = logits[:, -1, :] # becomes (B, C)\n","        # apply softmax to get probabilities\n","        probs = F.softmax(logits, dim=-1) # (B, C)\n","        # sample from the distribution\n","        initiator_token_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","        # append sampled index to the running sequence\n","        initiator_token = torch.cat((initiator_token, initiator_token_next), dim=1) # (B, T+1)\n","    return initiator_token\n","\n","############## Creating a Bigram Model (It is named Ngram here though but it is Bigram !!!!!) ###################\n","\n","\n","\n","# Create a Bigram Language Model Object\n","NgramLM = NgramLanguageModel()\n","\n","# print the number of parameters in the model\n","print(sum(p.numel() for p in NgramLM.parameters())/1e6, 'M parameters')\n","\n","# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(NgramLM.parameters(), lr=1e-3)\n","\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    NgramLM.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits = NgramLM.forward(X)\n","            Loss = NgramLM.LossFunction(logits,Y)\n","            losses[k] = Loss.item()\n","        out[split] = losses.mean()\n","    NgramLM.train()\n","    return out\n","\n","\n","########## TRAINING LOOP ###########\n","\n","for iter in range(max_iters):\n","\n","  # every once in a while evaluate the loss on train and val sets\n","  if iter % eval_interval == 0 or iter == max_iters - 1:\n","      losses = estimate_loss()\n","      print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","  # get a random batch of data\n","  xb, yb = get_batch('train')\n","\n","  # forward pass\n","  logits = NgramLM.forward(xb)\n","\n","  # Calculate loss\n","  Loss = NgramLM.LossFunction(logits,yb)\n","\n","  #print(NgramLM.channels.weight.grad)\n","  #Zero all parameter gradients\n","  optimizer.zero_grad(set_to_none=True)\n","  #\n","  #print(NgramLM.channels.weight.grad)\n","\n","  # Backward Path to Calculate new grads\n","  Loss.backward()\n","\n","  # Update the weights in embedding\n","  optimizer.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lQ_jWXMWGyqC","executionInfo":{"status":"ok","timestamp":1709656554389,"user_tz":300,"elapsed":127,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"e8029103-cdc4-4e0d-bafb-8ec1af71d04d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'train': 2, 'eval': 3}\n"]}]}]}