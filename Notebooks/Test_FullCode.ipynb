{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMpWRu29auZimZGd4MmCrpw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YuggAmI-Xzxr","executionInfo":{"status":"ok","timestamp":1709693617867,"user_tz":300,"elapsed":118295,"user":{"displayName":"Aref Vandadi","userId":"09114108971880870091"}},"outputId":"7585716e-e4b6-4c07-f5fa-61130b9d6cd4"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-03-06 02:51:43--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘input.txt’\n","\n","\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n","\n","2024-03-06 02:51:43 (19.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n","\n","0.042369 M parameters\n","step 0: train loss 4.3020, val loss 4.3183\n","step 100: train loss 3.2595, val loss 3.2858\n","step 200: train loss 3.0590, val loss 3.0241\n","step 300: train loss 2.8736, val loss 2.9166\n","step 400: train loss 2.7563, val loss 2.8486\n","step 500: train loss 2.7195, val loss 2.7461\n","step 600: train loss 2.6396, val loss 2.6781\n","step 700: train loss 2.6213, val loss 2.6236\n","step 800: train loss 2.5600, val loss 2.5928\n","step 900: train loss 2.5664, val loss 2.5490\n","step 1000: train loss 2.5550, val loss 2.5030\n","step 1100: train loss 2.5160, val loss 2.5344\n","step 1200: train loss 2.5087, val loss 2.5454\n","step 1300: train loss 2.5238, val loss 2.5033\n","step 1400: train loss 2.5102, val loss 2.5230\n","step 1500: train loss 2.5021, val loss 2.4871\n","step 1600: train loss 2.4685, val loss 2.4836\n","step 1700: train loss 2.4618, val loss 2.4438\n","step 1800: train loss 2.4313, val loss 2.4666\n","step 1900: train loss 2.4487, val loss 2.4427\n","step 2000: train loss 2.4543, val loss 2.4405\n","step 2100: train loss 2.4216, val loss 2.4559\n","step 2200: train loss 2.4002, val loss 2.4552\n","step 2300: train loss 2.3821, val loss 2.4066\n","step 2400: train loss 2.3835, val loss 2.4327\n","step 2500: train loss 2.3958, val loss 2.3901\n","step 2600: train loss 2.3935, val loss 2.4014\n","step 2700: train loss 2.3733, val loss 2.4144\n","step 2800: train loss 2.4012, val loss 2.4188\n","step 2900: train loss 2.4008, val loss 2.3837\n","step 3000: train loss 2.3655, val loss 2.3651\n","step 3100: train loss 2.3577, val loss 2.3823\n","step 3200: train loss 2.3722, val loss 2.3622\n","step 3300: train loss 2.3553, val loss 2.3619\n","step 3400: train loss 2.3699, val loss 2.3451\n","step 3500: train loss 2.3510, val loss 2.3342\n","step 3600: train loss 2.3492, val loss 2.3618\n","step 3700: train loss 2.3741, val loss 2.3495\n","step 3800: train loss 2.3432, val loss 2.3219\n","step 3900: train loss 2.2997, val loss 2.3436\n","step 4000: train loss 2.3190, val loss 2.3285\n","step 4100: train loss 2.2962, val loss 2.3166\n","step 4200: train loss 2.2992, val loss 2.3336\n","step 4300: train loss 2.3191, val loss 2.3297\n","step 4400: train loss 2.3226, val loss 2.3070\n","step 4500: train loss 2.2782, val loss 2.3302\n","step 4600: train loss 2.2748, val loss 2.3240\n","step 4700: train loss 2.3069, val loss 2.3242\n","step 4800: train loss 2.3114, val loss 2.3328\n","step 4900: train loss 2.2916, val loss 2.3381\n","step 4999: train loss 2.2871, val loss 2.2927\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","################ Initial Hyperparameters ###########\n","## Hyperparameter ##\n","#number of embedding for each character --> i.e. C channels entering Self-Attention Head\n","n_embed = 32\n","# number of batches for each iteration\n","batch_size = 4\n","# How many characters are used to predict the next? Context = 1 --> Bigram model, Context > 1 --> Ngram model\n","context = 8\n","# number of Self-Attention heads\n","num_heads = 4\n","# size of Self-Attention head\n","head_size = n_embed//num_heads\n","# number of Transformer (Multi-head Self-Attention + Feed Forward) layers\n","n_layer = 3\n","# dropout\n","dropout = 0.2\n","# Max Number of Iteration to Train the Model\n","max_iters = 5000\n","# Loss Evaluation Interval\n","eval_interval = 100\n","# Number of batches to Mean over for Loss evaluation at every interval\n","eval_iters = 200\n","################ Initial Hyperparameters ###########\n","\n","################ Initial Hyperparameters Multiplied by 4 ###########\n","# ## Hyperparameter ##\n","# #number of embedding for each character --> i.e. C channels entering Self-Attention Head\n","# n_embed = 128\n","# # number of batches for each iteration\n","# batch_size = 16\n","# # How many characters are used to predict the next? Context = 1 --> Bigram model, Context > 1 --> Ngram model\n","# context = 32\n","# # number of Self-Attention heads\n","# num_heads = 16\n","# # size of Self-Attention head\n","# head_size = n_embed//num_heads\n","# # number of Transformer (Multi-head Self-Attention + Feed Forward) layers\n","# n_layer = 12\n","# # dropout\n","# dropout = 0.2\n","# # Max Number of Iteration to Train the Model\n","# max_iters = 5000\n","# # Loss Evaluation Interval\n","# eval_interval = 100\n","# # Number of batches to Mean over for Loss evaluation at every interval\n","# eval_iters = 200\n","################ Initial Hyperparameters Multiplied by 2 ###########\n","\n","\n","\n","torch.manual_seed(1337)\n","\n","\n","# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n","!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","\n","# Opening the tinyshakspeare Book\n","with open(\"input.txt\", 'r', encoding='utf-8') as tinyshkspr:\n","  book = tinyshkspr.read()\n","\n","r = sorted(set(book))\n","chars =''.join(r)\n","# Number of unique characters in the model\n","charsize = len(chars)\n","\n","## I used rfind() function to create my encoder and deccoder instead of using dictionary which was used in the Original code\n","encoder = lambda c: [chars.rfind(c[i]) for i in range(len(c))]\n","decoder = lambda c: \"\".join([chars[i] for i in c])\n","\n","# Encode the Book (Type = Python List)\n","book_digits_List=encoder(book)\n","# Convert Encoded Book from python List to PyTorch Tensor\n","book_digits = torch.tensor(book_digits_List)\n","\n","# Creating Training and Evaluation Data\n","n=len(book_digits)*9//10\n","Train_data = book_digits[:n]\n","Val_data = book_digits[n:]\n","\n","# Define a function to grab random batches from either Training or Evaluation data\n","def get_batch(x):\n","  data = Train_data if x == 'train' else Val_data\n","  Batch_start = torch.randint(0, len(data)-context,(batch_size,))\n","  xb = torch.stack([data[Batch_start[i]:Batch_start[i]+context] for i in range(batch_size)])\n","  yb = torch.stack([data[Batch_start[i]+1:Batch_start[i]+context+1] for i in range(batch_size)])\n","  return xb, yb\n","\n","############# Creating a Transformer Block as Seen in Attention is All You Need paper for a Bigram model ###############\n","\n","# A Class definition for Each Self-Attetion Block\n","class Head(nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","    self.key = nn.Linear(n_embed,head_size, bias=False)\n","    self.query = nn.Linear(n_embed,head_size, bias=False)\n","    self.value = nn.Linear(n_embed,head_size, bias=False)\n","    self.register_buffer('tril', torch.tril(torch.ones(context, context)))\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self,x):\n","    B,T,C = x.shape\n","    k = self.key(x) # Batch (B) X (T)_Entry X head_size (C) --- since This will be applied to the output of token_emb + pos_emb which is Batch (B) X (T)_Entry X n_embed (C)\n","    q = self.query(x) # Batch (B) X (T)_Entry X head_size (C) --- since This will be applied to the output of token_emb + pos_emb which is Batch (B) X (T)_Entry X n_embed (C)\n","\n","    qk = q @ k.transpose(-2,-1) * C**0.5 ## (B ,T ,C) X (B, C, T) = (B, T, T)\n","    wei = torch.ones(T,T)\n","    wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n","    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","    wei = F.softmax(wei, dim=-1) # (B, T, T)\n","    wei = self.dropout(wei) # dropout some of the information from previous nodes (I don't know why!!)\n","    # perform the weighted aggregation of the values\n","    v = self.value(x) # Batch (B) X (T)_Entry X head_size (C) --- since This will be applied to the output of token_emb + pos_emb which is Batch (B) X (T)_Entry X n_embed (C)\n","    out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n","    return out # each head takes n_embed and outputs head_size (C) --> Batch (B) X (T)_Entry X head_size (C)\n","\n","# A Class definition to Bring Multiple Attention blocks together and create a Multihead Attention Block\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head() for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embed,n_embed) ## Projection Layer back to the residual path: To me it only helps to match the dimensions to the residual path if different\n","        self.dropout = nn.Dropout(dropout) # introducing dropout rigt before it is added back to the residual path\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out)) ## Projection Layer back to the residual path: To me it only helps to match the dimensions to the residual path if different\n","        return out # outputs n_embed ---> by concatenating all the head_size self-attention Heads outputs\n","\n","class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embed, 4*n_embed), # In Attention is All You Need paper the hidden layers of the Feed Forward are 4 times the input and output (Page 5 of paper)\n","            nn.ReLU(),\n","            nn.Linear(4*n_embed, n_embed), ## Projection Layer back to the residual path: To me it only helps to match the dimensions to the residual path if different\n","            nn.Dropout(dropout) # introducing dropout rigt before it is added back to the residual path\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block: communication block (Multi-Head Self Attention) followed by computation blcok (FeedForward) \"\"\"\n","\n","    def __init__(self):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        self.sa = MultiHeadAttention()\n","        self.ffwd = FeedFoward()\n","        self.ln1 = nn.LayerNorm(n_embed)\n","        self.ln2 = nn.LayerNorm(n_embed)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x)) ## the addition of x in this argument is Resudial Connection. ## Layer norm is applied before Self Attention\n","        x = x + self.ffwd(self.ln2(x)) ## the addition of x in this argument is Resudial Connection ## Layer norm is applied before Feed Forward\n","        return x\n","#\n","#\n","#\n","############## Creating a Bigram Model (It is named Ngram here though but it is Bigram !!!!!) ###################\n","\n","class NgramLanguageModel(nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","    self.channels = nn.Embedding(charsize,n_embed)\n","    self.pos_embedding = nn.Embedding(context,n_embed) # positional embedding which will be the same for all entries in the batch i.e. for each context\n","    #self.SelfAttnHead = Head()\n","    self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n","    self.ln_final = nn.LayerNorm(n_embed) # Another Layer norm right before the last nn.Linear layer\n","    #self.SelfAttnHead = MultiHeadAttention() # takes n_embd and outputs n_embd\n","    #self.ffw = FeedFoward() # takes n_embd and outputs n_embd --> Allows the NN to think about the communication between tokens coming from Self-Sttention before generating Logits\n","    self.lm_head = nn.Linear(n_embed,charsize)\n","\n","  def forward(self,random_training_batch):\n","    # random_training_batch argument is generated by getbatch function (what we are calling xb in this code)\n","    B,T = random_training_batch.shape\n","    # looks up in the Embedding table created in the constructor to assign weights to each character coming in\n","    token_emb = self.channels(random_training_batch) # Batch (B) X (T)_Entry X n_embed (C)\n","    pos_emb = self.pos_embedding(torch.arange(T)) # (T)_Entry X n_embed (C)\n","    x = token_emb + pos_emb # pos_emb is getting broadcasted across all Batches or first dimension ---> Batch (B) X (T)_Entry X n_embed (C)\n","    x = self.blocks(x) # applies blocks of Multi-Head self-attention and feedforward\n","    x = self.ln_final(x)\n","    logits = self.lm_head(x) # Batch (B) X (T)_Entry X charsize (C=65)\n","    return logits\n","\n","  def LossFunction(self,logits,random_training_batch_nextChar):\n","    logits = logits.view(-1,charsize) # we are doing this since Pytorch functinal.cross_entropy function needs Channels to be assigned to the second dimension\n","    Target = random_training_batch_nextChar.view(-1)\n","    Loss = F.cross_entropy(logits,Target)\n","    return Loss\n","\n","  def generate(self, initiator_token, max_new_tokens):\n","    # idx is (B, T) array of indices in the current context\n","    for _ in range(max_new_tokens):\n","        #crop the input to the generator so it is not larger than context size since positional embedding defined above only accpepts values up to context (T)\n","        initiator_token_cropped = initiator_token[:,-context:]\n","        # get the predictions\n","        logits = self.forward(initiator_token_cropped)\n","        # focus only on the last time step\n","        logits = logits[:, -1, :] # becomes (B, C)\n","        # apply softmax to get probabilities\n","        probs = F.softmax(logits, dim=-1) # (B, C)\n","        # sample from the distribution\n","        initiator_token_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","        # append sampled index to the running sequence\n","        initiator_token = torch.cat((initiator_token, initiator_token_next), dim=1) # (B, T+1)\n","    return initiator_token\n","\n","############## Creating a Bigram Model (It is named Ngram here though but it is Bigram !!!!!) ###################\n","\n","\n","\n","# Create a Bigram Language Model Object\n","NgramLM = NgramLanguageModel()\n","\n","# print the number of parameters in the model\n","print(sum(p.numel() for p in NgramLM.parameters())/1e6, 'M parameters')\n","\n","# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(NgramLM.parameters(), lr=1e-3)\n","\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    NgramLM.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits = NgramLM.forward(X)\n","            Loss = NgramLM.LossFunction(logits,Y)\n","            losses[k] = Loss.item()\n","        out[split] = losses.mean()\n","    NgramLM.train()\n","    return out\n","\n","\n","########## TRAINING LOOP ###########\n","\n","for iter in range(max_iters):\n","\n","  # every once in a while evaluate the loss on train and val sets\n","  if iter % eval_interval == 0 or iter == max_iters - 1:\n","      losses = estimate_loss()\n","      print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","  # get a random batch of data\n","  xb, yb = get_batch('train')\n","\n","  # forward pass\n","  logits = NgramLM.forward(xb)\n","\n","  # Calculate loss\n","  Loss = NgramLM.LossFunction(logits,yb)\n","\n","  #print(NgramLM.channels.weight.grad)\n","  #Zero all parameter gradients\n","  optimizer.zero_grad(set_to_none=True)\n","  #\n","  #print(NgramLM.channels.weight.grad)\n","\n","  # Backward Path to Calculate new grads\n","  Loss.backward()\n","\n","  # Update the weights in embedding\n","  optimizer.step()"]},{"cell_type":"code","source":[],"metadata":{"id":"zw-FHt8NkdDM"},"execution_count":null,"outputs":[]}]}